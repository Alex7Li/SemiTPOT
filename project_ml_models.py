# -*- coding: utf-8 -*-
"""Project ML Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcgqfKcW_5BSCRxJmcLxjSZ0FYQkeN3p

# Install Dependancies
"""
import os
# Load all datasets locally
if not os.path.exists("./TweeBankDataset/"):
    print("Unzipping tweetbank")
    ! unzip -n /content/drive/MyDrive/IDL_RAIG/TweeBankDataset/Tweebank-dev.zip -d ./TweeBankDataset/

# TPANN Dataset
if not os.path.exists("./TPANNDataset"):
    print("Unzipping TPANN")
    ! unzip /content/drive/MyDrive/IDL_RAIG/TPANNDataset/TPANN_data.zip -d ./TPANNDataset/

if not os.path.exists("./ArkDataset"):
  print("Copying Ark")
  ! mkdir ArkDataset
  ! cp /content/drive/MyDrive/IDL_RAIG/ArkTwitter/* ./ArkDataset/

if not os.path.exists("./Results"):
  os.mkdir('Results')

! pip install pyyaml==5.4.1 transformers tokenizers datasets seqeval conllu emoji folium
! pip install -U kaleido

# Keep alphabetically ordered to minimize duplicate imports
from conllu import parse
from conllu import parse_incr
import csv
from datasets import load_dataset
import datetime
from functools import partial
from datasets import load_metric
import gc
import matplotlib.pyplot as plt
from torch.optim import AdamW
import numpy as np
import os
import pandas as pd
from plotly.tools import FigureFactory as ff
import seaborn as sn

import transformers
from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, BertForTokenClassification, AutoTokenizer, get_scheduler
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm as std_tqdm
tqdm =partial(std_tqdm, leave=False, position=0, dynamic_ncols=True)
from typing import *
import warnings
import zipfile

# Needed Imports
warnings.filterwarnings('ignore')

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Device: ", device)

"""# Dataloaders

## Ark Dataset
"""

# Ark twitter annotation guidelines --> https://github.com/brendano/ark-tweet-nlp/blob/master/docs/annot_guidelines.pdf (25 tags)
# ARK Twitter dataset
ark_train_path = "/content/ArkDataset/daily547.conll"
ark_val_path = "/content/ArkDataset/oct27.traindev"
ark_test_path = "/content/ArkDataset/oct27.test"

def read_ark_file(file_path: str) -> Tuple[List[str], List[str]]:
  """
  Process a data file from ARK twitter into words and their POS tags
  The word at the i-th index corresponds to the i-th POS
  --------------------------------------------------------------------
  Args
  -----
  file_path: str
    The full path to the data file

  Returns
  --------
  X: List[str]
    The words in the data file

  Y: List[str]
    The POS tags in the data file
  """
  with open(file_path, "r") as f:
    lines = f.read().splitlines()
    lines = [line.split("\t") for line in lines]
    X = [[]]
    Y = [[]]
    for line in lines:
      if len(line) != 2:
        X.append([])
        Y.append([])
        continue

      X[-1].append(line[0])
      Y[-1].append(line[1])

  return X, Y

def create_pos_mapping(ark_pos_tags: List[str]) -> Tuple[Dict[str, int], Dict[int, str]]:
  """
  Creates two dictionaries
  - The pos tags are mapped to their indices in the list and vice versa
  ----------------------------------------------------------------------
  Args
  -----
  ark_pos_tags: List[str]
    The list of pos tags used in the ark dataset

  Returns
  --------
  ark_pos_index_mapping: Dict[str, int]
    The mapping of pos to their indices

  ark_index_pos_mapping: Dict[int, str]
    The mapping of the indices to their corresponding pos
  """
  ark_pos_index_mapping = {tag:i for i, tag in enumerate(ark_pos_tags)}
  ark_index_pos_mapping = {v:k for k, v in ark_pos_index_mapping.items()}
  return ark_pos_index_mapping, ark_index_pos_mapping

def read_ark_file_test(ark_train_path):
  train_X, train_Y = read_ark_file(ark_train_path)
  assert train_X[1][1] == '@TheBlissfulChef'
  assert train_Y[2][0] == '!'

def ark_pos_mapping_test():
  ark_pos_tags = ["N", "O", "^", "S", "Z", "V", "A", "R",
                "!", "D", "P", "&", "T", "X", "#", "@",
                "~", "U", "E", "$", ",", "G", "L", "M", "Y"
                ]
  ARK_POS_INDEX_MAPPING, ARK_INDEX_POS_MAPPING = create_pos_mapping(ark_pos_tags)
  assert len(ARK_POS_INDEX_MAPPING) == 25
  assert len(ARK_INDEX_POS_MAPPING) == 25
  assert ARK_POS_INDEX_MAPPING["L"] == 22
  assert ARK_INDEX_POS_MAPPING[7] == "R"

read_ark_file_test(ark_train_path)
ark_pos_mapping_test()

class ArkDataset(torch.utils.data.Dataset):
    def __init__(self, data_path):
        self.data_path = data_path

        ARK_POS_TAGS = ["N", "O", "^", "S", "Z", "V", "A", "R",
                "!", "D", "P", "&", "T", "X", "#", "@",
                "~", "U", "E", "$", ",", "G", "L", "M", "Y"
                ]

        ARK_POS_INDEX_MAPPING, ARK_INDEX_POS_MAPPING = create_pos_mapping(ARK_POS_TAGS)
        self.ARK_POS_INDEX_MAPPING = ARK_POS_INDEX_MAPPING
        self.ARK_INDEX_POS_MAPPING = ARK_INDEX_POS_MAPPING

        self.X, self.Y_pos = read_ark_file(data_path)
        self.Y = []
        self.num_labels = len(ARK_POS_TAGS)

        for pos_tags in self.Y_pos:
          self.Y.append([])
          for pos_tag in pos_tags:
            self.Y[-1].append(self.ARK_POS_INDEX_MAPPING[pos_tag])
          
        assert(len(self.X) == len(self.Y))

    def __len__(self):
        return len(self.X)

    def __getitem__(self, ind):
        x = self.X[ind]
        y = self.Y[ind]
        y = torch.as_tensor(y, dtype=torch.long, device=device) 
        return x, y

    def collate_fn(batch):
        batch_x = [x for x,y in batch]
        batch_y = [y for x,y in batch]

        # convert all to tensors
        batch_x = [torch.as_tensor(x,dtype=torch.float64,device=device) for x,y in batch]
        batch_Y = [torch.as_tensor(y,dtype=torch.long,device=device) for x,y in batch]
        

        lengths_x = [x.shape[0] for x in batch_x]
        batch_x_pad =pad_sequence(batch_x, batch_first=True, padding_value=0.0)
        
        lengths_y = [y.shape[0] for y in batch_y]
        batch_y_pad = pad_sequence(batch_y, batch_first=True, padding_value=0)
        

        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)

ark_train = ArkDataset(ark_train_path)
ark_val = ArkDataset(ark_val_path)
ark_test = ArkDataset(ark_test_path)
print(len(ark_train) + len(ark_val) + len(ark_test))

"""## TweeBank Dataset"""

twee_train_path = "/content/TweeBankDataset/Tweebank-dev/en-ud-tweet-train.conllu"
twee_dev_path = "/content/TweeBankDataset/Tweebank-dev/en-ud-tweet-dev.conllu"
twee_test_path = "/content/TweeBankDataset/Tweebank-dev/en-ud-tweet-test.conllu"

def data_reader(data_path):
    """  Some useful info     
            https://rdrr.io/cran/NLP/man/CoNLLUTextDocument.html
            LEMMA (lemma or stem of word form), 
            UPOSTAG (universal part-of-speech tag, see https://universaldependencies.org/u/pos/index.html), 
            XPOSTAG (language-specific part-of-speech tag, may be 
    """
    with open(data_path, "r", encoding="utf-8") as data_file:
        X = []
        Y = []
        for tokenlist in parse_incr(data_file):
            X.append([])
            Y.append([])
            for token in tokenlist:
                X[-1].append(str(token))
                Y[-1].append(token['upos'])
    return X, Y

def make_pos_mapping():
    all_pos = set()
    def update_pos(tokenlist):
        for elem in tokenlist:
            all_pos.add(elem['upos'])
    
    def tokenz(data_path):
        with open(data_path, "r", encoding="utf-8") as data_file:
            for tokenlist in parse_incr(data_file):
                update_pos(tokenlist)
    tokenz(twee_dev_path)
    tokenz(twee_train_path)
    tokenz(twee_test_path)
    return dict(zip(list(all_pos),np.arange(len(list(all_pos)))))

TWEEBANK_POS_MAPPING = make_pos_mapping()


# Create Dataset Classes
class TweebankTrain(torch.utils.data.Dataset):
    def __init__(self, data_path): 
        self.Data_dir = data_path 
        self.X, self.Yraw = data_reader(data_path)
        self.Y = []
        self.num_labels = 17
        for ex in self.Yraw:
          self.Y.append([])
          for elem in ex:
            self.Y[-1].append(TWEEBANK_POS_MAPPING[elem])
          
        assert(len(self.X) == len(self.Y))

    def __len__(self):
        return len(self.X)

    def __getitem__(self, ind):
        X = self.X[ind]
        Y = self.Y[ind]
        Y = torch.as_tensor(Y, dtype=torch.long, device=device) 
        return X, Y
   
    def collate_fn(batch):

        batch_x = [x for x,y in batch]
        batch_y = [y for x,y in batch]

        
        # convert all to tensors
        batch_x = [torch.as_tensor(x,dtype=torch.float64,device=device) for x,y in batch]
        batch_Y = [torch.as_tensor(y,dtype=torch.long,device=device) for x,y in batch]
        

        lengths_x = [x.shape[0] for x in batch_x]# TODO: Get original lengths of the sequence before padding
        batch_x_pad =pad_sequence(batch_x, batch_first=True, padding_value=0.0) # TODO: pad the sequence with pad_sequence (already imported)
        
        lengths_y = [y.shape[0] for y in batch_y]# TODO: Get original lengths of the sequence before padding
        batch_y_pad = pad_sequence(batch_y, batch_first=True, padding_value=0)# TODO: pad the sequence with pad_sequence (already imported)
        

        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)
def get_data():
    dev_data = "/content/TweeBankDataset/Tweebank-dev/en-ud-tweet-dev.conllu"
    train_data = "/content/TweeBankDataset/Tweebank-dev/en-ud-tweet-train.conllu"
    test_data = "/content/TweeBankDataset/Tweebank-dev/en-ud-tweet-test.conllu"
    tweebank_train = TweebankTrain(twee_train_path)
    tweebank_val = TweebankTrain(twee_dev_path)
    tweebank_test = TweebankTrain(twee_test_path)
    return tweebank_train, tweebank_val, tweebank_test
tweebank_train, tweebank_val, tweebank_test = get_data()
print(len(tweebank_train) + len(tweebank_val) + len(tweebank_test))

"""## TPANN Dataset

"""

#code for testing
dev_data = "/content/TPANNDataset/dev.txt"
train_data = "/content/TPANNDataset/train.txt"
test_data = "/content/TPANNDataset/test.txt"
# all_pos = set()
# all_pos_without_punctuation = set()

#Helper function to keep track of all pos tags across the datasets
# punctuations = '''!()-[]{};:''""\,<>./?@#$%^&*_~"..'''
# def update_pos(tokenlist, ignore_punctuation=False):
#   for elem in tokenlist:
#     if ignore_punctuation and elem not in punctuations:
#       all_pos_without_punctuation.add(elem)
    
#     all_pos.add(elem)

# PTB_pos_tags_list = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']

def TPANN_data_reader(data_path, test_mode = False):
  """  Read txt files"""
  i = 0 
  with open(data_path, "r", encoding="utf-8") as data_file:
    
    X = list()

    if not test_mode:
      Y = list()

    temp_x,temp_y = list(), list()
    for i, line in enumerate(data_file):

      line = line.split()
      if len(line) == 0 and len(temp_x)!= 0:

        X.append(temp_x)
        temp_x = list()
        if not test_mode:
          Y.append(temp_y)
          # update_pos(temp_y)
          # update_pos(temp_y, ignore_punctuation=True)
          temp_y = list()


      elif len(line) == 2:
        temp_x.append(line[0])
        if not test_mode:
          if line[1] == "NONE":
            print(f"{i}: {line}")
          temp_y.append(line[1])
        

    if test_mode:
      return X
    
    return X, Y

class TPANNDataset(torch.utils.data.Dataset):
    def __init__(self, data_path, test_mode = False):
        self.data_path = data_path

        TPANN_POS_TAGS = [':', 'VBZ', 'PRP$', 'WRB', 'MD', 'RB', 'NNS', 'DT', 'UH', 'VBG', ']', 'NN', 'URL', 'VBD', '.', 'VBP', 'POS', 'WP', 'RT', 'VB', 'HT', ')', 'VBN', 'PRP', 'TO', 'NNP', 'JJR', 'USR', 'RP', 'SYM', ',', 'JJ', 'O', 'CC', "''", 'CD', '(', 'PDT', 'IN', '[', 'WDT', 'JJS', 'RBR', 'NNPS', 'LS', 'RBS', 'FW', 'EX']

        self.TPANN_POS_INDEX_MAPPING, self.TPANN_INDEX_POS_MAPPING = create_pos_mapping(TPANN_POS_TAGS)

        self.X, self.Y_pos = TPANN_data_reader(data_path, test_mode=test_mode)
        self.Y = []
        self.num_labels = len(TPANN_POS_TAGS)

        for pos_tags in self.Y_pos:
          self.Y.append([])
          for pos_tag in pos_tags:
            self.Y[-1].append(self.TPANN_POS_INDEX_MAPPING[pos_tag])
          
        assert(len(self.X) == len(self.Y))

    def __len__(self):
        return len(self.X)

    def __getitem__(self, ind):
        x = self.X[ind]
        y = self.Y[ind]
        y = torch.as_tensor(y, dtype=torch.long, device=device) 
        return x, y

    def collate_fn(batch):
        batch_x = [x for x,y in batch]
        batch_y = [y for x,y in batch]

        # convert all to tensors
        batch_x = [torch.as_tensor(x,dtype=torch.float64,device=device) for x,y in batch]
        batch_Y = [torch.as_tensor(y,dtype=torch.long,device=device) for x,y in batch]
        

        lengths_x = [x.shape[0] for x in batch_x]
        batch_x_pad =pad_sequence(batch_x, batch_first=True, padding_value=0.0)
        
        lengths_y = [y.shape[0] for y in batch_y]
        batch_y_pad = pad_sequence(batch_y, batch_first=True, padding_value=0)
        

        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)

tpann_train = TPANNDataset(train_data,test_mode=False)
tpann_val = TPANNDataset(dev_data,test_mode=False)
tpann_test = TPANNDataset(test_data,test_mode=False)

print(len(tpann_train) + len(tpann_val) + len(tpann_test))

"""# Loading Data"""

# https://github.com/huggingface/notebooks/blob/main/examples/token_classification.ipynb
def tokenize_and_align_labels(orig_x, orig_labels, tokenizer):
    """
    Tokenize x, and lengthen the original labels similarly.
    It handles start/end special tokens as well as cases where a word is
    split into many words.
    orig_x: A list of words
    orig_labels_example: A list of label indicies
    output: data formatted the way we want
    """
    tokenized_inputs = tokenizer(orig_x, truncation=True, is_split_into_words=True)

    word_ids = tokenized_inputs.word_ids()
    previous_word_idx = None
    label_ids = []
    for word_idx in word_ids:
        # Special tokens have a word id that is None. We set the label to -100 so they are automatically
        # ignored in the loss function.
        if word_idx is None:
            label_ids.append(PYTORCH_IGNORE_INDEX) 
        # We set the label for the first token of each word.
        else:
            label_ids.append(orig_labels[word_idx])
        previous_word_idx = word_idx
    tokenized_inputs['labels'] = label_ids
    return tokenized_inputs

class TransformerCompatDataset(torch.utils.data.Dataset):
    def __init__(self, orig_dataset:torch.utils.data.Dataset, tokenizer):
        self.orig_dataset = orig_dataset
        self.tokenizer = tokenizer

    def __getitem__(self, idx):
        words, labels = self.orig_dataset[idx]
        encoding = tokenize_and_align_labels(words, labels, self.tokenizer)
        return encoding

    def __len__(self):
        return len(self.orig_dataset)

def get_num_examples(dataloader):
    total_examples = 0
    for batch in dataloader:
        total_examples += len(batch)
    return total_examples

def map_index_to_tag(indices, mapping):
  """
  Map an index to the appropriate pos tag using the mapping for a dataset
  ------------------------------------------------------------------------
  Args
  -----
  1. indices: List[int]
    List of integers where each int corresponds to a pos tag
  2. mapping: Dict[int, str]
    Dataset specific mapping of int to pos tag

  Returns
  --------
  The actual pos tags
  """

  return [mapping[index] for index in indices]

def map_labels_to_twee(pos_tags, mapping):
  """
  Map the pos tags of a dataset to the unified pos tags
  ------------------------------------------------------
  Args
  -----
  1. pos_tags: List[str]
    List of pos tags from a particular dataset
  2. mapping: Dict[str, str]
    Maps the pos tags from a dataset to the unified pos tags

  Returns
  --------
  Unified pos tags for the dataset
  """
  return [mapping[tag] for tag in pos_tags]

def filter_negative_hundred(preds, labels):
  """
  Flatten a list of tensors
  --------------------------
  Args
  -----
  1. preds: List[Tensor]
    A list of 2d tensor preds
  2. labels: List[Tensor]
    A list of 2d tensor label
  
  Returns
  --------
  Two lists (pred, label)
  """
  flattened_preds = [tensor.flatten() for tensor in preds]
  final_preds = torch.cat(flattened_preds, dim=0)
  final_preds = [tensor.item() for tensor in final_preds]

  flattened_labels = [tensor.flatten() for tensor in labels]
  final_labels = torch.cat(flattened_labels, dim=0)
  final_labels = [tensor.item() for tensor in final_labels]

  pred_labels = [pair for pair in zip(final_preds, final_labels) if pair[1] != -100]
  final_preds = [pair[0] for pair in pred_labels]
  final_labels = [pair[1] for pair in pred_labels]
  return final_preds, final_labels

def get_acc(preds, labels):
  """
  Get the accuracy of unified predictions
  ----------------------------------------
  Args
  -----
  1. preds: List[str]
    Model predictions
  2. labels: List[str]
    The actual labels of the dataset

  Returns
  --------
  Accuracy of predictions on the dataset
  """
  num_correct = 0
  for i in range(len(preds)):
    if preds[i] == labels[i]:
      num_correct += 1

  return num_correct / len(preds)

def get_dataset_mapping(dataset_name):
  """
  Get the index to pos mapping for the original dataset
  Get the dataset pos to unified pos mapping
  ------------------------------------------------------
  Args
  -----
  1. dataset_name: str
    The dataset that the model making predictions has been trained on
  
  Returns
  --------
  1. index_pos_mapping: Dict[int, str]
    Maps the integer predictions to their pos tag for dataset_name
  2. dataset_to_twee_pos_mapping: Dict[str, str]
    Maps the pos tags for dataset_name to unified pos tags
  """
  if dataset_name == "ark":
    ARK_POS_TAGS = ["N", "O", "^", "S", "Z", "V", "A", "R",
            "!", "D", "P", "&", "T", "X", "#", "@",
            "~", "U", "E", "$", ",", "G", "L", "M", "Y"
            ]
    pos_index_mapping, index_pos_mapping = create_pos_mapping(ARK_POS_TAGS)
    dataset_to_twee_pos_mapping = {
        "N": "NOUN", "O": "PRON", "^": "PROPN", "S": "PRON", "Z": "PROPN", "V": "VERB",
        "A": "ADJ", "R": "ADV", "!": "INTJ", "D": "DET", "P": "ADP", "&": "CCONJ",
        "T": "ADP", "X": "DET", "#": "X", "@": "X", "~": "PUNCT", "U": "X", "E": "SYM",
        "$": "NUM", ",": "PUNCT", "G": "X", "L": "AUX", "M": "PROPN", "Y": "DET"
    }
    assert set(ARK_POS_TAGS) == set(dataset_to_twee_pos_mapping)
  elif dataset_name == "tweebank":
    # all_pos is run from above as a global
    pos_index_mapping = TWEEBANK_POS_MAPPING
    index_pos_mapping = {v:k for k, v in pos_index_mapping.items()}
    dataset_to_twee_pos_mapping = {v:v for v in index_pos_mapping.values()}
    assert set(TWEEBANK_POS_MAPPING.keys()) == set(dataset_to_twee_pos_mapping)
  elif dataset_name == "TPANN":
    TPANN_POS_TAGS = [':', 'VBZ', 'PRP$', 'WRB', 'MD', 'RB', 'NNS', 'DT', 'UH', 'VBG', ']',
                      'NN', 'URL', 'VBD', '.', 'VBP', 'POS', 'WP', 'RT', 'VB', 'HT', ')', 'VBN',
                      'PRP', 'TO', 'NNP', 'JJR', 'USR', 'RP', 'SYM', ',', 'JJ', 'O', 'CC',
                      "''", 'CD', '(', 'PDT', 'IN', '[', 'WDT', 'JJS', 'RBR', 'NNPS',
                      'LS', 'RBS', 'FW', 'EX', "WP$"
                      ]
    dataset_to_twee_pos_mapping = {'CD':'NUM', 'LS':'NUM','NNP':'PROPN','NNPS':'PROPN', 'SYM':'SYM',
                 'JJ':'ADJ','JJR':'ADJ','JJS':'ADJ', 'RB':'ADV','WRB':'ADV',
                 'RBR':'ADV','RBS':'ADV', 'DT':'DET', 'PDT':'DET','EX':'DET','WDT':'PRON',
                 'FW':'X','URL':'X','USR':'X','HT':'X','RT':'X', 'POS':'PART','TO':'PART',
                 'RP':'ADP', 'UH':'INTJ', 'PRP':'PRON','WP':'PRON','WP$':'PRON',
                 'CC':'CCONJ', 'IN':'ADP', 'NN':'NOUN','NNS':'NOUN', 'O':'PUNCT', 'MD':'AUX',
                 'VB':'VERB','VBD':'VERB','VBG':'VERB','VBN':'VERB','VBP':'VERB','VBZ':'VERB',
                 ":": "PUNCT", "]": "PUNCT", "''": "PUNCT", "(": "PUNCT", "[": "PUNCT", ")": "PUNCT",
                 ",": "PUNCT", "PRP$": "PRON", ".": "PUNCT"
                 }

    assert set(TPANN_POS_TAGS) == set(dataset_to_twee_pos_mapping)
    pos_index_mapping, index_pos_mapping = create_pos_mapping(TPANN_POS_TAGS)
  else:
    index_pos_mapping = None
    dataset_to_twee_pos_mapping = None
    print("no dataset")

  return index_pos_mapping, dataset_to_twee_pos_mapping

"""# Train Setup"""

def train_epoch(model, train_dataloader, optimizer, scheduler):
    model.train()
    for batch in tqdm(train_dataloader):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

def get_validation_acc(preds, labels, train_dataset_name, val_dataset_name):
    train_index_pos_mapping, train_dataset_to_twee_pos_mapping = get_dataset_mapping(train_dataset_name)

    pos_preds = map_index_to_tag(preds, train_index_pos_mapping)
    unified_pos_preds = map_labels_to_twee(pos_preds, train_dataset_to_twee_pos_mapping)

    val_index_pos_mapping, val_dataset_to_twee_pos_mapping = get_dataset_mapping(val_dataset_name)
    pos_true = map_index_to_tag(labels, val_index_pos_mapping)
    unified_pos_labels = map_labels_to_twee(pos_true, val_dataset_to_twee_pos_mapping)

    return get_acc(unified_pos_preds, unified_pos_labels)

def validation_epoch(model, val_dataloader):
    model.eval()
    preds = []
    labels = []
    for batch in tqdm(val_dataloader):
        batch = {k: v.to(device) for k, v in batch.items()}
        batch_labels = batch['labels']
        del batch['labels']
        with torch.no_grad():
            outputs = model(**batch)
    
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        # correct += torch.sum((batch["labels"] == predictions).float())
        # total += batch["labels"].shape[0] * batch["labels"].shape[1]
        # total -= torch.sum(batch["labels"] == -100)
        preds.append(predictions)
        labels.append(batch_labels)
    return filter_negative_hundred(preds, labels)

PYTORCH_IGNORE_INDEX = -100 # Index ignored by pytorch https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html
def get_dataset(model_name, dataset_name, batch_size, partition):
    assert partition in {'train', 'val', 'test'}
    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, use_fast=True)
    if model_name == 'gpt2':
        tokenizer.pad_token = tokenizer.eos_token
    data_collator = DataCollatorForTokenClassification(tokenizer)
    if partition == 'train':
        if dataset_name == 'tweebank':
            dataset = tweebank_train
        elif dataset_name == 'ark':
            dataset = ark_train
        elif dataset_name == 'TPANN':
            dataset = tpann_train
        else:
            raise NotImplementedError
    elif partition == 'val':
        if dataset_name == 'tweebank':
            dataset = tweebank_val
        elif dataset_name == 'ark':
            dataset = ark_val
        elif dataset_name == 'TPANN':
            dataset = tpann_val
        else:
            raise NotImplementedError
    elif partition == 'test':
        if dataset_name == 'tweebank':
            dataset = tweebank_test
        elif dataset_name == 'ark':
            dataset = ark_test
        elif dataset_name == 'TPANN':
            dataset = tpann_test
        else:
            raise NotImplementedError
    else:
        raise NotImplementedError
    dataloader = DataLoader(TransformerCompatDataset(dataset, tokenizer), shuffle=False, batch_size=batch_size, collate_fn=data_collator)
    return dataloader, dataset.num_labels

def load_model(model_name, num_labels):
    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)
    return model.to(device)

def training_loop(hparams):
    torch.cuda.empty_cache()
    train_dataloader, num_labels = get_dataset(hparams['model_name'], hparams['dataset'], hparams['batch_size'], 'train')
    val_dataloader, _ = get_dataset(hparams['model_name'], hparams['dataset'], hparams['batch_size'], 'val')
    n_epochs = hparams['n_epochs']
    model = load_model(hparams['model_name'], num_labels)
    optimizer = AdamW(model.parameters(), lr=5e-5)

    lr_scheduler = get_scheduler(
        name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=get_num_examples(train_dataloader)*n_epochs
    )
    val_accs = []
    if not os.path.exists('models'):
        os.mkdir('models')
    best_model_path = os.path.join('models', hparams['model_name'] + "_" + hparams['dataset'])
    torch.save(model.state_dict(), best_model_path)
    best_val_acc = 0
    for i in tqdm(range(0, n_epochs)):
        train_epoch(model, train_dataloader, optimizer, lr_scheduler)
    
        preds, labels = validation_epoch(model, val_dataloader)
        val_acc = get_validation_acc(preds, labels,  hparams["dataset"], hparams['dataset'])
        val_accs.append(val_acc)
        if val_acc > best_val_acc:
            torch.save(model.state_dict(), best_model_path)
        print(f"Val Accuracy Train epoch {i+1}: {round(100*val_acc,3)}%")
        
    if hparams['n_epochs'] > 1:
        # Make ranadeep happy
        plt.xlabel('epoch')
        plt.ylabel('accuracy')
        plt.ylim([.4, 1])
        plt.title(f"Training curve for: {hparams['model_name']}")
        plt.plot(range(n_epochs), val_accs)
        plt.show()
    model.load_state_dict(torch.load(best_model_path))  
    return model

def invert_permutation(perm):
    inverse = [0] * len(perm)
    for i, p in enumerate(perm):
        inverse[p] = i
    return inverse

def plot_label_confusion(preds, labels, train_dataset_name, val_dataset_name):
    n_label_classes = np.max(labels) + 1
    n_pred_classes = np.max(preds) + 1
    train_name_map, train_to_unified  = get_dataset_mapping(train_dataset_name)
    val_name_map, val_to_unified  = get_dataset_mapping(val_dataset_name)

    sorted_pred_integer_classes = sorted(range(n_pred_classes), key = lambda i: train_to_unified[train_name_map[i]])
    sorted_label_integer_classes = sorted(range(n_label_classes), key = lambda i: val_to_unified[val_name_map[i]])
    train_reorder = invert_permutation(sorted(range(n_pred_classes), key = lambda i: train_to_unified[train_name_map[i]]))
    val_reorder = invert_permutation(sorted(range(n_label_classes), key = lambda i: val_to_unified[val_name_map[i]]))
    confusion_matrix = np.zeros((n_label_classes, n_pred_classes))
    for l, p in zip(labels, preds):
        confusion_matrix[val_reorder[l]][train_reorder[p]] += 1
    # Negate incorrect labels so that we can actually read the thing
    for l in range(n_label_classes):
        for p in range(n_pred_classes):
            if val_to_unified[val_name_map[l]] != train_to_unified[train_name_map[p]]:
                confusion_matrix[val_reorder[l]][train_reorder[p]] *= -1
    assert(np.sum(confusion_matrix) > 0)  # Make sure the sorting is correct, we should have good accuracy
    df_cm = pd.DataFrame(confusion_matrix,
                    index = [val_name_map[invert_permutation(val_reorder)[l]] for l in range(n_label_classes)],
                    columns = [train_name_map[invert_permutation(train_reorder)[p]] for p in range(n_pred_classes)])
    plt.figure(figsize = (n_pred_classes*3//4, n_label_classes*3//4))
    sn.heatmap(df_cm, annot=True, fmt='g')
    plt.show('output.png')

def get_model_predictions_and_true_labels(hparams, val_dataset_name):
    model = training_loop(hparams)

    val_dataloader, _ = get_dataset(hparams['model_name'], val_dataset_name, hparams['batch_size'], 'val')
    preds, labels = validation_epoch(model, val_dataloader)

    return preds, labels

preds_tpann, labels_tpann = get_model_predictions_and_true_labels({
    'n_epochs': 3,
    'batch_size': 8,
    'dataset': 'TPANN',
    'model_name': 'roberta-large',
}, 'tweebank')

plot_label_confusion(preds_tpann, labels_tpann, 'TPANN', 'tweebank')

model_names = [
    'gpt2',
    'vinai/bertweet-large',
    'roberta-large',
    'bert-large-cased',
]
dataset_names = [
    'tweebank',
    'TPANN',
    'ark'
]

def run_experiment():
    result_dict = dict()
    for model_name in model_names:

        result_dict[model_name] = dict()

        for train_dataset_name in dataset_names:

            result_dict[model_name][train_dataset_name] = dict()

            hparams = {
                'n_epochs': 10,
                'batch_size': 8,
                'dataset': train_dataset_name,
                'model_name': model_name
            }

            print(f"Training on: {train_dataset_name}, with model: {model_name}")
            trained_model = training_loop(hparams)
            
            for test_dataset_name in dataset_names:
                print(f"Validating: {test_dataset_name}, with model: {model_name}, trained on: {train_dataset_name}")
                val_dataloader, _ = get_dataset(model_name, test_dataset_name, hparams['batch_size'], 'test')
                preds, labels = validation_epoch(trained_model, val_dataloader)
                acc = get_validation_acc(preds, labels,  train_dataset_name, test_dataset_name)
                print(f"Test Accuracy on {test_dataset_name}: {round(100*acc,3)}%")
                result_dict[model_name][train_dataset_name][test_dataset_name] = round(100*acc,3)

    return result_dict

"""# Train Model!"""

results = run_experiment()
# results = {'gpt2': {'tweebank': {'tweebank': 88.605, 'TPANN': 70.083, 'ark': 55.007}, 'TPANN': {'tweebank': 62.676, 'TPANN': 86.848, 'ark': 58.427}, 'ark': {'tweebank': 47.659, 'TPANN': 64.866, 'ark': 86.512}}, 'vinai/bertweet-large': {'tweebank': {'tweebank': 93.93, 'TPANN': 72.831, 'ark': 58.798}, 'TPANN': {'tweebank': 67.468, 'TPANN': 93.794, 'ark': 63.418}, 'ark': {'tweebank': 50.942, 'TPANN': 68.416, 'ark': 93.649}}, 'roberta-large': {'tweebank': {'tweebank': 92.997, 'TPANN': 71.411, 'ark': 57.999}, 'TPANN': {'tweebank': 71.133, 'TPANN': 92.837, 'ark': 63.047}, 'ark': {'tweebank': 52.045, 'TPANN': 67.397, 'ark': 92.689}}, 'bert-large-cased': {'tweebank': {'tweebank': 91.77, 'TPANN': 68.13, 'ark': 53.976}, 'TPANN': {'tweebank': 65.373, 'TPANN': 90.571, 'ark': 60.942}, 'ark': {'tweebank': 50.051, 'TPANN': 63.793, 'ark': 90.931}}}
print(results)

"""# Plot Results"""

# CONFUSION MATRIX 
# import plotly.figure_factory as ff
#rows = ['trained on d1','model trained on d2','model trained on d3']
# cols = ['d1','d2','d3']

def matrix_to_str(matrix):
    return np.array(matrix).astype(str).copy()

def plot_confusion(confusion_matrix, dataset_names, given_title='title'):
    cf_matrix_list_str = matrix_to_str(confusion_matrix)
    # print(f"cf_matrix_list_str: {cf_matrix_list_str}")
    fig = ff.create_annotated_heatmap(confusion_matrix, x=dataset_names, y=dataset_names , annotation_text=cf_matrix_list_str, autocolorscale=True, range_color=[.5,1])
    #https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap
    # add title
    fig.update_layout(title_text=("Model: " + given_title),
                      xaxis = dict(title='Validated On'),
                      yaxis = dict(title='Trained On')
                     )

    # add custom xaxis title
    fig.add_annotation(dict(font=dict(color="black",size=14),
                            x=0.5,
                            y=-0.15,
                            showarrow=False,
                            text="Predicted value",
                            xref="paper",
                            yref="paper"))

    # add custom yaxis titl
    fig.add_annotation(dict(font=dict(color="black",size=14),
                            x=-0.35,
                            y=0.5,
                            showarrow=False,
                            text="Real value",
                            textangle=-90,
                            xref="paper",
                            yref="paper"))

    # adjust margins to make room for yaxis title
    fig.update_layout(margin=dict(t=50, l=200))

    # add colorbar
    fig['data'][0]['showscale'] = True
    fig.write_image('./Results/' + given_title + '.png')
    fig.show()

#Make dictionary into needed list
matrix = list()
# model_names = ['bert-large-cased']
# rows = ["tweebank", "TPANN", "ark"]
for model_used in results.keys():
  matrix.append(list())
  for dataset_trained_on in results[model_used].keys():
    matrix[-1].append(list())
    for dataset_validated_on, acc in results[model_used][dataset_trained_on].items():
      # print(f"model_used: {model_used}, dataset_trained_on: {dataset_trained_on}, dataset_validated_on: {dataset_validated_on}, acc: {results[model_used][dataset_trained_on][dataset_validated_on]}%")
      matrix[-1][-1].append(acc)

  plot_confusion(matrix[-1], dataset_names, given_title=model_used.split('/')[-1])
  print()